# Доказательства

### 1. Сформулировать и доказать теорему о методе Гаусса

Всякую конечную матрицу можно с помощью элементарных образований к ступенчатому виду.

#### Доказательство

$\square$ Предъявим алгоритм. Фиксируем текущий элемент в левом верхнем углу ($a_{11}$).

1. Если текущей элемент равен нулю, переходим к пункту $2$. Иначе называем текущий элемент ($a_{ij}$) ведущим. Используя ведущую строку добиваемся того, чтобы все элементы ниже и выше $a_{ij}$ были равны $0$, а на месте $a_{ij}$ стояла единица:

$$
\forall k \neq j \Rightarrow A_k := A_k - A_j \frac{a{kj}}{a{ij}} \

A_i := A_i \frac{1}{a_{ij}}
$$

Выбираем новый текущий элемент, смещаясь в матрице на столбец вправо и на строку вниз.

2. Если текущий элемент равен $0$, то просматриваем все элементы под ним. Если среди них существует ненулевой, меняем местами строку, содержащую ненулевой элемент, с текущей, иначе переходим к пунту 3.
3. Если текущий элемент и все под ним равны $0$, то смещаемся на столбец вправо и переходим к пункту 1.

В том случае, когда мы достигли последнего столбца, прекращаем работу алгоритма.

Так как матрица конечна, то за конечное число шагов алгоритм завершит работу, приведя матрицу к ступенчатому (каноническому) виду. $\blacksquare$

### 2. Выписать формулы крамера для квадратной матрицы произвольного порядка и доказать их (предполагается, что решение существует, а свойства определителя известны)

Для системы $A X = B$

$x_k = \frac{\Delta_k}{\Delta}$, где $\Delta$ — определитель матрицы $A$, а $\Delta_i$ — определитель матрицы, получаемой из матрицы $A$ заменой $i$-го столбца на $B$.



$\square$ Т. к. $\Delta \neq 0$, матрица $A$ обратима, следовательно

$$
X = A^{-1}B = \frac{1}{\det A} \left( 
\begin{array}{cccc}
A_{11} & A_{21} & \dots & A_{n1} \\
A_{12} & A_{22} & \dots & A_{n2} & \\
\vdots & \vdots & \ddots & \vdots &  \\
A_{1n} & A_{2n} & \dots & A_{nn} & 
\end{array}
 \right) \left(
\begin{array}{c}
b_1 \\
b_2 \\
\vdots \\
b_n
\end{array}
\right),
$$

откуда

$$
x_k = \frac{1}{\det A} \sum^n_{i=1} A_{ik} b_i = \frac{1}{\det A} (b_1 A_{1k} + b_2 A_{2k} + \dots + b_n A_{nk}), \\
k = 1, 2, \dots, n.
$$

Разложив $\Delta_k$ по $k$-му столбцу, получаем:

$$
\Delta_k = b_1 A_{1k} + b_2 A_{2k} + \dots + b_n A_{nk},
$$

следовательно

$$
x_k = \frac{\Delta_k}{\Delta}. \blacksquare
$$

### 3. Дать определение обратной матрицы. Сформулировать и доказать критерий существования обратной матрицы. Единственна ли обратная матрица? Ответ обосновать.

$A^{-1}$ — обратная к $A$, если

$$
A^{-1} A = A A^{-1} = E.
$$

Обратная матрица единственна. 

$\square$ Допустим, $A^{-1}$ и $\widetilde{A^{-1}}$ — две разные обратные матрицы к матрице $A$. Тогда в силу ассоциативности

$$
 A^{-1}  = A^{-1} E = A^{-1} A \widetilde{A^{-1}} = E \widetilde{A^{-1}} = \widetilde{A^{-1}},
$$

откуда

$$
A^{-1} = \widetilde{A^{-1}}.
$$

Пришли к противоречию. $\blacksquare$

Матрица обратима тогда и только тогда, когда она невырождена (ее определитель не равен нулю).

$\square$ $1)$ См. вопрос 4.

$2)$ Допустим, $A A^{-1} = E$. Тогда

$$
\det A A^{-1} = \det A \det A^{-1} = \det E = 1 \neq 0,
$$

следовательно, 

$$
\det A \neq 0. \blacksquare
$$

### 4. Выписать формулу для нахождения обратной матрицы и доказать её.

$$
A^{-1} = \frac{1}{|A|} A^*,
$$

где $A^*$ — союзная матрица.

$\square$ Допустим, $|A| \neq 0$. Построим союзную матрицу к $A$:

$$
A^* = \left(
\begin{array}{cccc}
A_{11} & A_{21} & \dots & A_{n1} & \\
A_{12} & A_{22} & \dots & A_{n2} & \\
\vdots & \vdots & \ddots & \vdots & \\
A_{1n} & A_{2n} & \dots & A_{nn} & \\
\end{array}
\right).
$$

Найдем элементы матрицы $A A^* = (b_{ij})$:

$$
b_{ij} = a_{i1} A_{j1} + a_{i2} A_{j2} + \dots + a_{in} A_{jn}.
$$

Легко видеть, что $b_{ii}$ — разложение $|A|$ по $i$-й строке, а $b_{ij}$ (при $i \neq j$) — фальшивое разложение, следовательно

$$
A A^* = \left(
\begin{array}{cccc}
|A| & 0 & \dots & 0 \\
0 & |A| & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots & \\
0 & 0 & \dots & |A| \\ 
\end{array}
\right) = |A| E.
$$

Повторяя аналогичные рассуждения для матрицы $A^* A$, получаем

$$
A A^* = A^* A = |A| E
$$

откуда, т. к. $|A| \neq 0$, следует

$$
A \left( \frac{1}{|A|} A^* \right) = \left( \frac{1}{|A|} A^* \right) A = E,
$$

следовательно,

$$
\frac{1}{|A|} A^* = \frac{1}{|A|} A^* = A^{-1}.
$$

###5. Сформулировать критерий линейной зависимости и доказать его.

Система векторов линейно зависима тогда и только тогда, когда один из векторов линейно выражается через другие.

$\square$ $1)$ Допустим,

$$
v_n = \alpha_1 v_1 + \alpha_2 v_2 + \dots + \alpha_{n-1} v_{n-1},
$$

тогда

$$
\alpha_1 v_1 + \alpha_2 v_2 + \dots + \alpha_{n-1} v_{n-1} - v_n = 0,
$$

следовательно, система векторов $v_1, v_2, \dots, v_n$ линейно зависима, т. к. в разложении нуля есть хотя бы один ненулевой коэфициент ($-1$ при $v_n$).

$2)$ Допустим, система векторов $v_1, v_2, \dots, v_n$ линейно зависима. Тогда 

$$
\alpha_1 v_1 + \alpha_2 v_2 + \dots + \alpha_{n-1} v_{n-1} + \alpha_n v_n = 0,
$$

и хотя бы один из $\alpha_1, \dots, \alpha_n$ не равен нулю. Допустим, $\alpha_n \neq 0$. Тогда

$$
v_n = \frac{\alpha_1}{\alpha_n} v_1 + \frac{\alpha_2}{\alpha_n} v_2 + \dots + \frac{\alpha_{n-1}}{\alpha_n} v_{n-1}. \blacksquare
$$

###6. Дать определение ранга. Как он меняется при транспонировании? Ответ обосновать, используя только определение.

Ранг матрицы — это порядок ее базисного минора. (Определение Chernyshew-style — прим. г.)

Минор называется базисным, если он ненулевой и имеет порядок $i$, а все миноры порядка $i+1$ нулевые или не существуют.

При транспонировании ранг матрицы не меняется.

$\square$ Допустим, матрица $A$ порядка имеет ненулевой базисный минор $M = \det A^{i_1, \dots, i_k}_{j_1, \dots, j_k}$. Тогда матрица $A^T$ будет иметь базисный минор $\widetilde{M} = \det (A^T)^{i_1, \dots, i_k}_{j_1, \dots, j_k} = \det A^{j_1, \dots, j_k}_{i_1, \dots, i_k}$.

Легко видеть, что минор $\widetilde{M}$ отличается от $M$ операцией транспонирования. Так как при транспонировании определитель не меняется, их определители оба ненулевые. Следовательно,
$$
\text{rank} A^T \ge \text{rank} A.
$$

Повторяя рассуждения, получаем

$$
\text{rank} A = \text{rank} (A^T)^T \ge \text{rank} A^T \ge \text{rank}A,
$$

а следовательно, 

$$
\text{rank} A^T = \text{rank} A. \blacksquare
$$

### 7. Сформулировать и доказать критерий невырожденности квадратной матрицы, использующий понятие ранга (теорема о базисном миноре предполагается известной)

Квадратная матрица невырождена тогда и только тогда, когда ее ранг равен ее размерности.

$\square$ Рассмотрим квадратную матрицу размерности $n$.

$1)$ Допустим, матрица невырождена. В качестве базисного минора возьмем определитель всей матрицы. Размерность этого базисного минора равна $n$. Так как ранг — это размерность базисного минора, в таком случае ранг также равен $n$.

$2)$ Допустим, ранг матрицы равен $n$. Значит, размерность базисного минора также равна $n$, следовательно, базисный минор равен определителю матрицы. Так как минор ненулевой, определитель матрицы тоже ненулевой, и матрица невырождена. $\blacksquare$

### 8. Выписать свойства решений однородных и неоднородных СЛАУ (линейная комбинация решений однородной СЛАУ является решением однородной СЛАУ, разность двух решений неоднородной СЛАУ есть решение однородной СЛАУ, сумма решения однородной СЛАУ и решения неоднородной СЛАУ есть решение неоднородной СЛАУ) и доказать их.

Линейная комбинация решений однородной СЛАУ является решением однородной СЛАУ.

$\square$ Допустим, $A X_1 = 0$, и $A X_2 = 0$. Тогда

$$
A (\alpha X_1 + \beta X_2) = \alpha A X_1 + \beta A X_2 = 0. \blacksquare
$$

Разность двух решений неоднородной СЛАУ есть решение однородной СЛАУ.

$\square$ Допустим, $A X_1 = B$, и $A X_2 = B$. Тогда

$$
A (X_1 - X_2) = A X_1 - A X_2 = B - B = 0. \blacksquare
$$

Сумма решения однородной СЛАУ и решения неоднородной СЛАУ есть решение неоднородной СЛАУ.

$\square$ Допустим, $A X_1 = 0$, и $A X_2 = B$. Тогда

$$
A (X_1 + X_2) = A X_1 + A X_2 = 0 + B = B. \blacksquare
$$

### 9. Сформулировать теорему Кронекера-Капелли и доказать её.

СЛАУ совместна тогда и только тогда, когда ранг ее основной матрицы равен рангу расширенной.

$\square$ $1)$ Если система $A \begin{pmatrix} x_1\\ \vdots \\ x_n \end{pmatrix} = B$ совместна, то вектор $B$ — линейная комбинация векторов-столбцов матрицы $A$ с коэффициентами $x_1, \dots, x_n$, следовательно, количество линейно независимых столбцов в матрицах $A$ и $A|B$ одинаковое.

Так как ранг матрицы равен количеству линейно независимых столбцов, 

$$
\text{rank} A = \text{rank} (A|B).
$$

$2)$ Допустим $\{ A^1, \dots, A^j \}$ — максимальный линейно независимый набор столбцов матрицы $A$. Т. к. $\text{rank} A = \text{rank} (A|B)$, набор $\{ A^1, \dots, A^j, B \}$ будет уже линейно зависимым. Следовательно (там по одной теореме у Кострикина), $B$ — линейная комбинация векторов $A^1, \dots, A^j$. $\blacksquare$

### 10. Сформировать критерий существования ненулевого решения однородной системы линейных уравнений с квадратной матрицей и доказать его.

Однородная СЛАУ с квадратной матрицей имеет ненулевое решение тогда и только тогда, когда ее матрица вырождена.

$\square$ Существование ненулевого решения у системы $AX = 0$ равносильно существованию разложения нулевого вектора на векторы-столбцы матрицы $A$ с ненулевыми коэфициентами. Другими словами, ненулевое решение существует тогда, когда столбцы матрицы $A$ линейно зависимы.

По теореме о базисном миноре, столбы базисного минора линейно независимы и другие столбы матрицы является линейными комбинациями столбцов базисного минора. Вследствие этого,

$1)$ Если столбцы линейно зависимы, определитель матрицы $A$ не является базисным минором и равен нулю — матрица вырождена.

$2)$ Если матрица вырождена — ее определитель равен нулю, тогда он не является базисным минором и какие-то из столбцов матрицы линейно выражаются через другие — столбцы матрицы линейно зависимы. $\blacksquare$

### 11. Дать определение фундаментальной системы решений (ФСР) однородной системы решений. Доказать теорему о существовании ФСР.

Любой набор из $n - r$ линейно независимых решений СЛАУ $AX = 0$, где $n$ — кол-во неизвестных, $r = \text{rank} A$.

Пусть дана СЛАУ $AX = 0$ с $n$ неизвестными и $\text{rank} A = r$. Тогда существует набор из $k = n - r$ решений $x^{(1)}, \dots, x^{(k)}$ этой СЛАУ, образующих ФСР.

$\square$ Будем считать, что базисный минор матрицы $A$ находится в левом верхнем углу, т.е. в столбцах $1, \dots, r$ и строках $1, \dots, r$.

По теореме о базисном миноре уравнения с $r + 1$ по $n$ являются линейными комбинация первых $r$ уравнений. Значит, их можно отбросить без изменения решения системы. Получаем систему

$$
\begin{cases}
    a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n = 0 \\
    a_{21}x_1 + a_{22}x_2 + \dots + a_{2n}x_n = 0\\
    \dots \\
    a_{r1}x_1 + a_{r2}x_2 + \dots + a_{rn}x_n = 0. \\
\end{cases}
$$

Разделим переменные на базисные $x_1, \dots, x_r$ и свободные $x_{r+1}, \dots, x_n$. Перенесем свободные в правую часть:

$$
\begin{cases}
    a_{11}x_1 + \dots + a_{1r}x_r = - a_{1(r+1)} x_{r+1} - \dots - a_{1n} x_n \\
    a_{21}x_1 + \dots + a_{2r}x_r = - a_{2(r+1)} x_{r+1} - \dots - a_{2n} x_n \\
    \dots \\
    a_{r1}x_1 + \dots + a_{rr}x_r = - a_{r(r+1)} x_{r+1} - \dots - a_{rn} x_n. \\
\end{cases}
$$

Если мы присвоим произвольные значения свободным переменным, то относительно базисных мы получим квадратную СЛАУ с невырожденной матрицей, решение которой существует и единственно. Таким образом, решение исходной СЛАУ однозначно определяется значениями свободных неизвестных $x_{r+1}, \dots, x_n$. Рассмотрим следующие $k = n - r$ серий значений свободных неизвестных:

$$
\begin{array}
& x^{(1)}_{r+1} = 1,  & x^{(2)}_{r+1} = 0, & \dots & x^{(k)}_{r+1} = 0, \\
x^{(1)}_{r+2} = 0, & x^{(2)}_{r+} = 1, & \dots & x^{(k)}_{r+2} = 0, \\
\vdots & \vdots & \ddots & \vdots & \\
x^{(1)}_n = 0, & x^{(2)}_n = 0, & \dots & x^{(k)}_n = 1.
\end{array}
$$

$i$-й серии значений серии значений свободных переменных однозначно соответствуют значения $x_1^{(i)}, \dots, x_r^{(i)}$ базисных. Покажем, что столбцы:

$$
x^{(i)} = \begin{pmatrix}
x^{(i)}_1\\
\vdots \\
x^{(i)}_n
\end{pmatrix}, i = \overline{1, k}
$$

образуют ФСР. Т. к. эти столбцы по построению являются решениями системы $AX = B$ и их количество равно $k = n - r$, остается доказать их линейную независимость.

Допустим, существуют $\alpha_1, \dots, \alpha_k такие, что$

$$
\alpha_1 x^{(1)} + \dots + \alpha_k x^{(k)} = 0.
$$

Но тогда $r+1$-й элемент левого вектора равен $\alpha_1 1 + \alpha_2 0 + \dots + \alpha_k 0 = 0$, из чего следует $\alpha_1 = 0$. Повторяя рассуждения для следующих элементов, приходим к выводу, что $\alpha_1 = \dots = \alpha_k = 0$. Следовательно, система линейно независима. $\blacksquare$

#### 12. Доказать теорему о структуре общего решения однородной СЛАУ (то есть, о том, что произвольное решение однородной СЛАУ может быть представлено в виде комбинации ФСР).

Если $x^{(1)}, \dots, x^{(k)}$ — произвольная ФСР системы $AX = 0$, то любое ее решение $x$ можно представить в виде

$$
x = c_1 x^{(1)} + \dots + c_k x^{(k)}.
$$

И наоборот, любая линейная комбинация векторов ФСР будет решением этой СЛАУ.

$\square$ $1)$ Что линейная комбинация векторов ФСР является решением, следует из того, что векторы ФСР является решениями и линейная комбинация решений является решением (См. вопрос 8).

$2)$ Допустим $x$ — решение СЛАУ.

Предположим, что базисный минор в однородной СЛАУ находится в левом-верхнем углу. Тогда запишем систему как

$$
\begin{cases}
a_{11} x_1 + \dots + a_{1r} x_r = - a_{1(r+1)} x_{r+1} + \dots + a_{1n} x_n \\
\vdots \\
a_{r1} x_1 + \dots + a_{rr} x_r = - a_{r(r+1)} x_{r+1} + \dots + a_{rn} x_n \\
\end{cases}
$$

и решим ее относительно базсиных переменных:

$$
x_1 = \overline{a}_{1(r+1)} x_{r+1} + \dots + \overline{a}_{1n} x_n, \\
\vdots \\
x_r = \overline{a}_{r(r+1)} x_{r+1} + \dots + \overline{a}_{rn} x_n,
$$

где $\overline{a}_{ij}$ — какие-то коэффициенты в каноническом виде матрицы про решении ее методом Гаусса.

Составим матрицу из столбцов ФСР и припишем к ним столбец $x$:

$$
H = \begin{pmatrix}
\phi_{11} & \dots & \phi_{1k} & x_1 \\
\phi_{21} & \dots & \phi_{2k} & x_2 \\
\vdots & \ddots & \vdots & \vdots \\
\phi_{r1} & \dots & \phi_{rk} & x_r \\
\vdots & \ddots & \vdots & \vdots \\
\phi_{n1} & \dots & \phi_{nk} & x_n \\
\end{pmatrix}.
$$

Найдем ранг этой матрицы. Т. к. первые $r$ столбцов линейно независимы, $\text{rank} H \ge n - r$.

Но так как все столбцы являются решениями системы и удовлетворяют полученному выше условию, каждая из первых $r$ *строк* является линейной комбинацией последних $n - r$ строк. Так что $\text{rank} H \le n - r$.

Следовательно, ранг матрицы равен $n - r = k$ и комбинация ее $k + 1$ столбцов линейно зависима. Так как ее столбцы без $x$ линейно независимы, а с $x$ — линейно зависимы, то $x$ является их линейной комбинацией. $\blacksquare$

#### 13. Сформулировать теорему о структуре общего решения неоднородной СЛАУ и доказать ее.

Если $x$ — решение системы $AX = B$, а $\phi^{(1)}, \dots, \phi^{(k)}$ — произвольная ФСР системы $AX = 0$, то $x$ представляется в виде:

$$
x = x^0 + c_1 \phi^{(1)} + \dots + c_k \phi^{(k)},
$$

где $x^0$ — частное решение $AX = B$.

$\square$ Т. к. $x$, $x^0$ — решения неоднородной СЛАУ, то $x - x^0$ — решение однородной:

$$
A(x - x^0) = Ax - Ax^0 = B - B = 0.
$$

А любое решение однородной СЛАУ представляется в виде линейной комбинации векторов ФСР. Следовательно, 

$$
x - x^0 = c_1 \phi^{(1)} + \dots + c_k \phi^{(k)}. \blacksquare
$$

#### 14. Выписать формулу для вычисления векторного произведения в координатах, заданных в ортонормированном базисе, и доказать ее.

$$
[a,b] = \begin{vmatrix}
\textbf{i} & \textbf{j} & \textbf{k} \\
a_x & a_y & a_z \\
b_x & b_y & b_z
\end{vmatrix}.
$$

$\square$ Докажем, что такое векторное произведение подходит под определение.

$1)$ Скалярное произведение $([a, b], a)$ равно нулю, т. к. определитель матрицы, у которой два столбца линейно зависимы, равен нулю:

$$
([a, b], a) = a_x \begin{vmatrix}
a_y & a_z \\
b_y & b_z
\end{vmatrix} - a_y \begin{vmatrix}
a_x & a_z \\
b_x & b_z
\end{vmatrix} + a_z \begin{vmatrix}
a_x & a_y \\
b_x & b_y
\end{vmatrix} = \begin{vmatrix}
a_x & a_y & a_z \\
a_x & a_y & a_z \\
b_x & b_y & b_z
\end{vmatrix}.
$$

Аналогично $([a, b], b) = 0$.

Следовательно, предложенное произведение ортогонально каждому из векторов $a, b$.

$2)$ Докажем, что $|[a,b]| = |a| |b| \sin \phi$.

$$
\cos\phi = \frac{(a,b)}{|a| |b|} \\
\sin^2 \phi = 1 - \cos^2\phi = \frac{|a|^2 |b|^2 - (a, b)^2}{|a|^2 |b|^2} \\
\sin \phi = \frac{\sqrt{|a|^2 + |b|^2 - (a, b)^2}}{|a| |b|},
$$

откуда

$$
|a| |b| \sin \phi = \sqrt{|a|^2 + |b|^2 - (a, b)^2} = \\
a_y^2 b_x^2 - 2 a_x a_y b_x b_y + a_x^2 b_y^2 + a_z^2 b_x^2 - 2 a_x a_z b_x b_z + a_x^2 b_z^2 + \\
+ a_z^2 b_y^2 - 2 a_y a_y b_y b_z + a_y^2 b_z^2.
$$

А модуль $[a,b]$

$$
|[a,b]| = |(\begin{vmatrix}
a_y & a_z \\
b_y & b_z
\end{vmatrix}, \begin{vmatrix}
a_x & a_z \\
b_x & b_z
\end{vmatrix}, \begin{vmatrix}
a_x & a_y \\
b_x & b_y
\end{vmatrix})| = \\
= |(a_y b_z - a_z b_y, a_z b_x - a_x b_z, a_x b_y - a_y b_x)|
$$

равен тому же самому. Вот так вот!

$3)$ По тому же самому свойству определителя

$$
[a, a] = \begin{vmatrix}
\textbf{i} & \textbf{j} & \textbf{k} \\
a_x & a_y & a_z \\
a_x & a_y & a_z \\
\end{vmatrix} = 0. \blacksquare
$$

#### 15. Доказать теорему о том, что любое линейное уравнение на координаты точки в трехмерном пространстве задает плоскость и что любая плоскость определяется линейным уравнением.

Любая плоскость задается уравнением первой степени на координаты точки и любое такое уравнение задает плоскость.

$\square$ $1)$ Пусть плоскость задана точкой $M_0(x_0,y_0,z_0)$, лежащей на ней, и вектором $\vec{n}(A, B, C)$, перпендикулярным ей. Тогда для точки $M(x,y,z)$, лежащей на плоскости, вектор $\vec{M_0 M}$ будет перпендикулярен $\vec{n}$:

$$
A(x - x_0) + B(y - y_0) + C(z - z_0) = 0.
$$

Раскрывая скобки получаем уравнение

$$
Ax + By + Cz + D = 0,
$$

где $D = - A x_0 - B y_0 - C z_0$ и хотя бы один из коэффициентов $A, B, C$ ненулевой.

Таким образом, плоскость задается полученным уравнением.

$2)$ То же самое в обратном порядке. Выбираем любую точку $M_0$ на плоскости и вектор $\vec{n} (A, B, C)$ и видим, что это уравнение эквивалентно вышеприведенному условию принадлежности плоскости. $\blacksquare$

#### 16. Выписать формулу для вычисления расстояния от точки до плоскости и привести ее вывод.

Если плоскости $L$ задана уравнением $Ax + By + Cz + D = 0$, то расстояние от точки $M$ до $L$ равно

$$
\rho(M, L) = \frac{|A M_x + B M_y + C M_z + D|}{\sqrt{A^2 + B^2 + C^2}}.
$$

$\square$ Рассмотрим произвольную точку $M^0$, лежащую на данной плоскости. Тогда расстояние от $M$ до плоскости равно длине ортогональной проекции вектора $\vec{MM^0}$ на вектор нормали этой плоскости

$$
\rho(M, L) =  \frac{(\vec{MM^0}, \vec{n})}{|\vec{n}|}.
$$

Т. к. $\vec{n} = (A, B, C)$, получаем

$$
\rho(M, L) =  \frac{(M_x - M^0_x) + (M_y - M^0_y) B + (M_z - M^0_z)C }{\sqrt{A^2 + B^2 + C^2}} = \\
= \frac{A M_x + B M_y + C M_z - A M^0_x - B M^0_y - C M^0_z}{\sqrt{A^2 + B^2 + C^2}}.
$$

Так как $M^0$ лежит на плоскости, ее координаты удовлетовряют уравнению плоскости, и

$$
A M^0_x + B M^0_y + C M^0_z = -D.
$$

Получаем 

$$
\rho(M, L) = \frac{A M_x + B M_y + C M_z + D}{\sqrt{A^2 + B^2 + C^2}}. \blacksquare
$$

#### 17. Выписать формулу для вычисления расстояния от точки до прямой и доказать ее.

Пусть прямая $L$ задана своим каноническим уравнением $\frac{x - x_0}{u} = \frac{y - y_0}{v} = \frac{z - z_0}{w}$. Тогда расстояние от точки $M$ до прямой $L$ равно

$$
\rho(M, L) = \frac{|[\vec{M M_0}, \vec{p}]|}{|\vec{p}|},
$$

где $M_0$ — произвольная точка на прямой ($(x_0, y_0, z_0)$ в каноническом уравнении), $\vec{p}$ — направляющий вектор прямой ($(u, v, w)$ в каноническом уравнении).

$\blacksquare$ Рассмотрим точку на прямой $M_0$. Длина векторного произведения вектора $\vec{M M_0}$ на направляющий вектор будет следующей:

$$
|[\vec{M M_0}, \vec{p}]| = |\vec{M M_0}| |\vec{p}| \sin \phi,
$$

где $\phi$ — угол между этими векторами.

Опустим перпендикуляр $M M_2$ из точки $M$ на прямую. Длина вектора $\vec{M M_2}$ и будет расстоянием от точки $M$ до прямой. Теперь рассмотрим прямоугольный треугольник $\triangle M M_1 M_2$. Так как $M M_1$ — гипотенуза, а $M M_2$ — противолежащий ей катет, то

$$
|M M_2| = |M M_1| \sin \phi.
$$

Следовательно,

$$
\rho(M, L) = |M M_2| = \frac{|[\vec{M M_0}, \vec{p}]|}{|\vec{p}|}. \blacksquare
$$

#### 18. Выписать формулу для вычисления расстояния между двумя скрещивающимися прямыми и доказать ее.

Если скрещивающиеся прямые $L_1$ и $L_2$ заданы своими каноническими уравнениями $\frac{x - x_0}{p} = \frac{y - y_0}{e} = \frac{z - z_0}{t}$ и $\frac{x - x_1}{u} = \frac{y - y_1}{k} = \frac{z - z_1}{h}$, то расстояние между ними равно

$$
\rho(L_1, L_2) = \frac{(\vec{M_0 M_1}, [\vec{p_1}, \vec{p_2}])}{[\vec{p_1}, \vec{p_2}]},
$$

где $\vec{p_1} = (p, e, t)$, $\vec{p_2} = (u, k, h)$ — направляющие векторы прямых, $M_0 = (x_0, y_0, z_0)$, $M_1 = (x_1, y_1, z_1)$ — точки на прямых.

$\square$ Расстояние между скрещивающимися прямыми $L_1$ и $L_2$ — это расстояние от прямой $L_1$ до плоскости, содержащей прямую $L_2$, параллельной обеим прямым. Вектор нормали этой плоскости будет таким:

$$
\vec{n} = [\vec{p_1}, \vec{p_2}].
$$

Т. к. расстояние от прямой до параллельной ей плоскости — это расстояние от любой точки на этой прямой до этой плоскости, вышеобозначенная точка $M_0$ лежит на прямой, а $M_1$ на плоскости, то расстоянием будет длина ортогональной проекции вектора $\vec{M_0 M_1}$ на вектор нормали этой плоскости. Получаем

$$
\frac{(\vec{M_0 M_1}, [\vec{p_1}, \vec{p_2}])}{[\vec{p_1}, \vec{p_2}]}. \blacksquare
$$

#### 19. Выписать формулу Муавра и доказать ее.

Для комплексного числа $z = |z| (\cos \phi + i \sin \phi)$

$$
\sqrt[n]{z} = \sqrt[n]{|z|}(\cos \frac{\phi + 2\pi k}{n} + i \sin \frac{\phi + 2\pi k}{n}), k = \overline{1, n-1}.
$$

$\square$ Пусть дано число $z = r(\cos \phi + i \sin \phi)$, а мы хотим найти число $z' = r'(\cos \phi' + i \sin \phi')$ такое, что $(z')^n = z$.

Так как при умножении комплексных чисел модули умножаются, а аргументы складываются, получаем равенство

$$
(z')^n = (r')^n(\cos (\phi' n ) + i \sin (\phi' n )) = z = r(\cos \phi + i \sin \phi).
$$

Решая уравнение

$$
(r')^n = r
$$

получаем

$$
r' = \sqrt[n]{r}.
$$

Решая систему уравнений

$$
\begin{cases}
\cos (\phi' n) = \cos \phi \\
\sin (\phi' n) = \sin \phi 
\end{cases}
$$

там короче по формулам расписываем:

$$
\cos x - \cos y = -2 \sin \frac{x + y}{2} \sin \frac{x - y}{2} \\
\sin x - \sin y = 2 \sin \frac{x - y}{2} \cos \frac{x + y}{2}
$$

и всё точь-в-точь получается! Мамой клянусь! $\blacksquare$

#### 20. Доказать три свойства изоморфизма групп: 1) нейтральный элемент переходит в нейтральный, 2) обратный элемент переходит в обратный, 3) обратное отображение тоже является изоморфизмом.

$\square$ $1)$ Докажем, что $f(e) = e$.

Пусть $f(e) = h \in H$. Тогда

$$
h^2 = f(e)^2 = f(e^2) = f(e) = h,
$$

откуда $h = e$.

$2)$ $f(a^{-1}) = f(a)^{-1}$, так как

$$
f(a) f(a^{-1}) = f(aa^{-1}) = f(e) = e.
$$

$3)$ Если $f$ — биекция, то и $f^{-1}$ — биекция. Покажем, что если $f$ сохраняет операцию, то и $f^{-1}$ будет сохранять.

Подставим в

$$
f^{-1}(a)f^{-1}(b) = f^{-1}(ab)
$$

$a = f(u)$, $b = f(v)$. Получим

$$
f^{-1}(f(u))f^{-1}(f(v)) = f^{-1}(f(u)f(v)) \\
f^{-1}(f(u))f^{-1}(f(v)) = f^{-1}(f(uv)) \\
uv = uv. \blacksquare
$$

(Это я сам придумал и че-то неубедительно — прим. г.)

#### 21. Сформулировать и доказать теорему Кэли.

Любая конечная группа изоморфна подгруппе группы подстановок.

$\square$ Пусть $(G, *)$ — группа. Рассмотрим элемент $g \in F$ и функцию $f_g: G \rightarrow G, f_g(x) = g * x$. $f_g$ — биекция, т.к.

* $f_g$ инъективно, т. к. $x \neq y \Rightarrow g * x \neq g * y$ (иначе в группе не было бы обратного элемента);
* $f_g$ суръективно, т. к. множество $G$ конечно.

Пусть $\circ$ — композиция подстановок. Если $f_g$ — подстановка, то $f_{g^{-1}}$ — обратная ей, т. к.

$$
(f_{g^{-1}} \circ f_g) = g^{-1} * g * x = e * x = x.
$$

Аналогично, $f_e$ — тождественная подстановка.

Кроме того, все множество функций $K = \{f_g | g \in G\}$ замкнуто относительно композиции:

$$
f_g(f_h(x)) = g * h * x = f_{gh} (x),
$$

где $gh \in G$, что значит, что $f_{gh} \in K$.

Таким образом, $K$ — подгруппа симметрической группы. Докажем, что $G$ изоморфно $K$.

Рассмотрим $T: G \rightarrow K, T(x) = f_x$. $T$ — гомоморфизм, т. к сохраняет операцию: $T(g) \circ T(h) = f_g \circ f_h = f_{g * h} = T(g * h)$. $T$ — биекция, так как:

* инъективно: $f_g(x) = f_{g'}(x) \Rightarrow g = f_g(x) * x^{-1} = f_{g'}(x) * x^{-1} = g'$;
* суръективно (следует из определения множества $K$).

Таким образом, $T$ — изоморфизм, а значит, $G$ изоморфно подгруппе симметрической группы $K$. $\blacksquare$

#### 22. Сформулировать и доказать теорему Лагранжа.

Если $G$ — конечная группа, $H$ — ее подгруппа, то

$$
|G| = |G : H| |H|.
$$

$\square$ Все смежные классы $gH$ имеют одинаковое число элементов, равное $|H|$. Поскольку они разбивают группу $G$ на классы эквивалентности, то имеем что имеем. $\blacksquare$

#### 23. Сформулировать и доказать теорему о гомоморфизме групп.

Пусть $f: G \rightarrow H$ — гомоморфизм групп. Тогда

$$
\text{Im} f \simeq G/\text{Ker}f.
$$

$\square$ $f(g_1) = f(g_2) \Leftrightarrow g_1 \equiv g_2 (\text{mod} \;\text{Ker} f)$. В частности, $f$ инъективно тогда и только тогда, когда $\text{Ker} f = \{e\}$. Действительно,

$$
f(g_1) = f(g_2) \Leftrightarrow f(g_1^{-1} g_2) = e \Leftrightarrow g_1^{-1} g_2 \in \text{Ker} f \Leftrightarrow g_1 \equiv g_2 (\text{mod} \; \text{Ker} f).
$$

Таким образом, гомоморфизм $f: G \rightarrow H$ является изоморфизмом тогда и только тогда, когда $\text{Im} f = H$ и $\text{Ker} f = \{e\}$.

Следовательно, все элементы смежного класса $g \text{Ker} f$, и только они, переходят в элементы $h = f(g) \in \text{Im} f$. Тем самым показано, что отображение $\phi$, о котором идет речь в теореме, определено и является биекцией. Осталось проверить, что $\phi$ — гомоморфизм.

Пусть $g_1, g_2 \in G, f(g_1) =  h_1, f(g_2) = h_2$. Тогда $f(g_1 g_2) = h_1 h_2$ и

$$
\phi (h_1 h_2) = g_1 g_2 \text{Ker} f = (g_1 \text{Ker} f) (g_2 \text{Ker} f) = \phi(h_1) \phi(h_2). \blacksquare
$$

#### 24. Выведите формулу для описания изменения координат вектора при изменении базиса.

$$
x_{e'} = T^{-1}_{e \rightarrow e'} x_e.
$$

$\square$ Допустим, $e = (e_1, \dots, e_n)$ — старый базис, $e' = (e'_1, \dots, e'_n)$ и

$$
e'_1 = a_{11} e_1 + \dots + a_{n1} e_n \\
\vdots \\
e'_n = a_{1n} e_1 + \dots + a_{nn} e_n.
$$

Тогда вектор $x$, имеющий в базисе $e$ координаты $x_e = (x_1, \dots, x_n)^T$, будет иметь в базисе $e'$ координаты $x_{e'} = (x'_1, \dots, x'_n)^T$:

$$
x = x'_1 e'_1 + \dots + x'_n e'_n = x'_1 (a_{11} e_1 + \dots + a_{n1} e_n) + \\ + \dots + x'_n (a_{1n} e_1 + \dots + a_{nn} e_n) = \\
= (x'_1 a_{11} + x'_2 a_{12} + \dots + x'_n a_{1n}) e_1 + \dots = \\
= x_1 e_1 + \dots + x_n e_n.
$$

Тогда

$$
x_1 = x'_1 a_{11} + x'_2 a_{12} + \dots + x'_n a_{1n} \\
\vdots
$$

и

$$
\begin{pmatrix}
x_1 \\
\vdots \\
x_n
\end{pmatrix} = T_{e \rightarrow e'} \begin{pmatrix}
x'_1 \\
\vdots \\
x'_n
\end{pmatrix},
$$

где $T_{e \rightarrow e'}$ — матрица перехода от базиса $e$ к $e'$:

$$
T_{e \rightarrow e'} = \begin{pmatrix}
a_{11} & \dots & a_{1n} \\
\vdots & \ddots & \vdots \\
a_{n1} & \dots & a_{nn} \\
\end{pmatrix}.
$$

откуда

$$
\begin{pmatrix}
x'_1 \\
\vdots \\
x'_n
\end{pmatrix} = T^{-1}_{e \rightarrow e'}
\begin{pmatrix}
x_1 \\
\vdots \\
x_n
\end{pmatrix}. \blacksquare
$$

#### 25. Сформулируйте и докажите теорему о том, что действие линейного оператора полностью определяется матрицей линейного оператора.

Действие оператора полностью определено, если известны образы векторов базиса:

$$
A x = \begin{pmatrix}
a_{11} & \dots & a_{1n} \\
\vdots & \ddots & \vdots \\
a_{n1} & \dots & a_{nn} 
\end{pmatrix} x,
$$

где $(a_{11}, \dots, a_{n1})^T, \dots, (a_{1n}, \dots, a_{nn})^T$ — образы векторов базиса.

$\square$ По свойству линейности линейного оператора:

$$
Ax = A(x_1 e_1 + \dots + x_n e_n) = x_1 A e_1 + \dots + x_n A e_n = \\
= (A e_1, \dots, A e_n) \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}.
$$

Ну и слева которая - это и есть матрица оператора. $\blacksquare$

#### 26. Докажите утверждение о формуле для преобразования матрицы оператора при замене базиса.

$$
A_{e'} = T^{-1}_{e \rightarrow e'} A_e T_{e \rightarrow e'}.
$$

$\square$

$$
A_{e'} x_{e'} = T^{-1}_{e \rightarrow e'} A_e x^e = T^{-1}_{e \rightarrow e'} A_e T^{-1}_{e \rightarrow e'} x^{e'}. \blacksquare
$$

#### 27. Сформулируйте и докажите утверждение о связи характеристического уравнения и спектра оператора.

Для того, чтобы число являлось собственным значением линейного оператор, необходимо и достаточно, чтобы оно было корнем характеристического уравнения $\det (A - \lambda E) = 0$.

$\square$ $1)$ Пусть $\lambda$ — собственное значение. Значит, существует вектор $x$ такой, что

$$
Ax = \lambda x.
$$

Тогда

$$
Ax = \lambda E x,
$$

где $E$ — тождественный оператор. Это равносильно

$$
(A - \lambda E) x = 0,
$$

что представляет собой запись однородной СЛАУ с матрицей $A - \lambda E$ порядка $n$. По предположению, эта система имеет ненулевое решение (собственный вектор $x$), а значит, $\det (A - \lambda E) = 0$.

$2)$ То же самое в обратном порядке. $\blacksquare$

#### 28. Каким свойством обладают собственные векторы линейного оператора, отвечающие различным собственным значениям? Ответ обоснуйте.

Система векторов $(e_1, \dots, e_n)$, соответствующих различным собственным значениям $\lambda_1, \dots, \lambda_n$, линейно независима.

$\square$ Доказываем по индукции по параметру $r$ — количеству векторов.

$1)$ При $r = 1$ вектор ненулевой, а значит система из него линейно независима.

$2)$ Пусть утверждение верно при $r = m$. Добавим к системе еще один собственный вектор $e_{m+1}$, отвечающий собственному значению $\lambda_{m+1}$. Предположим, что получившаяся система векторов линейно зависима:

$$
\alpha_1 e_1 + \dots + \alpha_m e_m + \alpha_{m+1} e_{m+1} = 0.
$$

Применим к обеим частям оператор, получится:

$$
\alpha_1 A e_1 + \dots + \alpha_m A e_m + \alpha_{m+1} A e_{m+1} = \\
= \alpha_1 \lambda_1 e_1 + \dots + \alpha_m \lambda_m e_m + \alpha_{m+1} \lambda_{m+1} e_{m+1}.
$$

Умножив первое на $\lambda_{m+1}$ и вычтя из него второе, получим:

$$
\alpha_1 (\lambda_1 - \lambda_{m+1}) e_1 + \dots + \alpha_m (\lambda_m - \lambda_{m+1}) e_m = 0,
$$

т. е. система векторов $(e_1, \dots, e_m)$ линейно зависима. Пришли к противоречию. $\blacksquare$

#### 29. Сформулируйте и докажите критерий диагональности матрицы оператора.

Матрица оператора диагональна, когда она определена в базисе собственных векторов этого оператора.

$\square$ Допустим, $(e_1, \dots, e_n)$ — базис собственных векторов оператора $\mathcal{A}$. Тогда матрица этого оператора в данном базисе (которая по определению состоит из образов векторов этого базиса, записанных по столбцам) будет иметь вид

$$
A = \mathcal{A}(e_1, \dots, e_n) = (\mathcal{A}e_1, \dots, \mathcal{A}e_n) = \\
= (\lambda_1 e_1, \dots, \lambda_n e_n),
$$

откуда

$$
A = \begin{pmatrix}
\lambda_1 & \dots & 0 \\
\vdots & \ddots & \vdots \\
0 & \dots & \lambda_n
\end{pmatrix}. \blacksquare
$$

#### 30. Каким свойством обладает оператор в $n$-мерном вещественном пространстве, у которого есть $n$ различных действительных корней? Ответ обосновать.

# 

#### 31. Выпишите и докажите неравенство Коши-Буняковского.

$$
|(x, y)| \leq \|x\| \|y\|.
$$

$\square$ Так как скалярное произведение положительно определено,

$$
\lambda^2 (x, x) - 2\lambda (x, y) + (y, y) = (\lambda x - y, \lambda x - y) \geq 0,
$$

где $\lambda$ — произвольное вещественное число.

Посмотрим на левую часть как на квадратный трехчлен $f(\lambda)$. Тогда ее дискриминант:

$$
D(f) = (2 (x, y))^2 - 4(x, x) (y, y).
$$

Так как $f(\lambda) \geq 0$ для всех $\lambda$, то $D(f) \leq 0$, откуда

$$
(2 (x, y))^2 \leq 4(x, x) (y, y).
$$

Разделим обе части на 2 и возьмем квадратные корни:

$$
(x, y) \leq \sqrt{(x, x)} \sqrt{(y, y)} = \|x\| \|y\|. \blacksquare
$$

#### 32. Сформулируйте и докажите критерий линейной зависимости с помощью матрицы грама.

Система векторов линейно зависима тогда и только тогда, когда определитель матрицы Грама этой системы равен нулю.

$\square$ Система векторов $e_1, \dots, e_n$ линейно зависима, значит, что существует такой набор ненулевых коэффициентов, что

$$
x_1 e_1 + \dots + x_n e_n = 0.
$$

Умножаем скалярно обе части сначала на $e_1$, потом на $e_2$, получим условие

$$
\Gamma (e_1, \dots, e_n) \begin{pmatrix}
x_1 \\
\vdots \\
x_n
\end{pmatrix} = 0,
$$

которое является однородной СЛАУ с матрицей $\Gamma (e_1, \dots, e_n)$ и нетривиальным решением $(x_1, \dots, x_n)^T$, что эквивалентно условию $\det \Gamma (e_1, \dots, e_n) = 0$. $\blacksquare$

